{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-core in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (1.29.3)\n",
      "Requirement already satisfied: requests>=2.18.4 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from azure-core) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from azure-core) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from azure-core) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from requests>=2.18.4->azure-core) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from requests>=2.18.4->azure-core) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from requests>=2.18.4->azure-core) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from requests>=2.18.4->azure-core) (2023.7.22)\n",
      "Requirement already satisfied: openai in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (0.27.9)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Requirement already satisfied: azure-search-documents in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (11.4.0b8)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.24.0 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from azure-search-documents) (1.29.3)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from azure-search-documents) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from azure-search-documents) (0.6.1)\n",
      "Requirement already satisfied: requests>=2.18.4 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-search-documents) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-search-documents) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-search-documents) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents) (2023.7.22)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: tenacity in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (8.2.3)\n",
      "Requirement already satisfied: openai[datalib] in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (0.27.9)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from openai[datalib]) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from openai[datalib]) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from openai[datalib]) (3.8.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from openai[datalib]) (1.25.2)\n",
      "Requirement already satisfied: pandas>=1.2.3 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from openai[datalib]) (2.0.3)\n",
      "Requirement already satisfied: pandas-stubs>=1.1.0.11 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from openai[datalib]) (2.0.3.230814)\n",
      "Requirement already satisfied: openpyxl>=3.0.7 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from openai[datalib]) (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from openpyxl>=3.0.7->openai[datalib]) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from pandas>=1.2.3->openai[datalib]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from pandas>=1.2.3->openai[datalib]) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)\n",
      "Requirement already satisfied: types-pytz>=2022.1.1 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from pandas-stubs>=1.1.0.11->openai[datalib]) (2023.3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from requests>=2.20->openai[datalib]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from requests>=2.20->openai[datalib]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from requests>=2.20->openai[datalib]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from requests>=2.20->openai[datalib]) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from aiohttp->openai[datalib]) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from aiohttp->openai[datalib]) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from aiohttp->openai[datalib]) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from aiohttp->openai[datalib]) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from aiohttp->openai[datalib]) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from aiohttp->openai[datalib]) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from tqdm->openai[datalib]) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ignac\\anaconda3\\envs\\azure-openai-charla\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.3->openai[datalib]) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install azure-core\n",
    "! pip install openai\n",
    "! pip install azure-search-documents --pre\n",
    "! pip install PyPDF2\n",
    "! pip install tenacity\n",
    "! pip install openai[datalib]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import PyPDF2\n",
    "import fnmatch\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import SearchIndex\n",
    "from azure.search.documents.models import Vector\n",
    "from azure.search.documents.indexes.models import (\n",
    "    ComplexField,\n",
    "    CorsOptions,\n",
    "    SearchIndex,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSettings,\n",
    "    VectorSearch,\n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    "    ScoringProfile,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key: 1c7f2...3613f\n",
      "OpenAI API base: https://wsl-openai-canada.openai.azure.com/\n",
      "OpenAI API version: 2023-03-15-preview\n",
      "OpenAI API type: azure\n",
      "Azure Search service name: wsl-cog-search-test-2\n",
      "Azure Search service key: 9GUM9...z1hrb\n",
      "Azure Search index name: cogsrch-index\n",
      "Azure Search endpoint: https://wsl-cog-search-test-2.search.windows.net/\n",
      "Azure Search vector config name: test-search-vector-config\n",
      "Azure Search semantic config name: test-search-semantic-config\n",
      "Data directory: E:/PDFs\n",
      "Index schema: ./index_schema.json\n"
     ]
    }
   ],
   "source": [
    "# Load secrets and config from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API\n",
    "openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "print(\"OpenAI API key: {}\".format(openai.api_key[:5] + '...' + openai.api_key[-5:]))\n",
    "print(\"OpenAI API base: {}\".format(openai.api_base))\n",
    "print(\"OpenAI API version: {}\".format(openai.api_version))\n",
    "print(\"OpenAI API type: {}\".format(openai.api_type))\n",
    "\n",
    "# Azure Search API\n",
    "search_service_name = os.getenv(\"SEARCH_SERVICE_NAME\")\n",
    "search_service_key = os.getenv(\"SEARCH_SERVICE_KEY\")\n",
    "search_index_name = os.getenv(\"SEARCH_INDEX_NAME\")\n",
    "search_endpoint = \"https://{}.search.windows.net/\".format(search_service_name)\n",
    "search_vector_config_name = os.getenv(\"SEARCH_VECTOR_CONFIG_NAME\")\n",
    "search_semantic_config_name = os.getenv(\"SEARCH_SEMANTIC_CONFIG_NAME\")\n",
    "print(\"Azure Search service name: {}\".format(search_service_name))\n",
    "print(\"Azure Search service key: {}\".format(search_service_key[:5] + '...' + search_service_key[-5:]))\n",
    "print(\"Azure Search index name: {}\".format(search_index_name))\n",
    "print(\"Azure Search endpoint: {}\".format(search_endpoint))\n",
    "print(\"Azure Search vector config name: {}\".format(search_vector_config_name))\n",
    "print(\"Azure Search semantic config name: {}\".format(search_semantic_config_name))\n",
    "\n",
    "# Other variables\n",
    "data_directory = os.getenv(\"FILEPATH_TO_DATA\")\n",
    "index_schema = os.getenv(\"FILEPATH_TO_INDEX_SCHEMA\")\n",
    "print(\"Data directory: {}\".format(data_directory))\n",
    "print(\"Index schema: {}\".format(index_schema))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a client\n",
    "class CreateClient(object):\n",
    "    def __init__(self, endpoint, key, index_name):\n",
    "        self.endpoint = endpoint\n",
    "        self.index_name = index_name\n",
    "        self.key = key\n",
    "        self.credentials = AzureKeyCredential(key)\n",
    "\n",
    "    # Create a SearchClient\n",
    "    # Use this to upload docs to the Index\n",
    "    def create_search_client(self):\n",
    "        return SearchClient(\n",
    "            endpoint=self.endpoint,\n",
    "            index_name=self.index_name,\n",
    "            credential=self.credentials,\n",
    "        )\n",
    "\n",
    "    # Create a SearchIndexClient\n",
    "    # This is used to create, manage, and delete an index\n",
    "    def create_admin_client(self):\n",
    "        return SearchIndexClient(endpoint=self.endpoint, credential=self.credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Search Index\n",
    "def create_schema_from_json_and_upload(index_name, admin_client):\n",
    "    cors_options = CorsOptions(allowed_origins=[\"*\"], max_age_in_seconds=60)\n",
    "    scoring_profiles = []\n",
    "    \n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "        SearchableField(name=\"filename\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"author\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"created_date\", type=SearchFieldDataType.DateTimeOffset),\n",
    "        SearchableField(name=\"last_modified_date\", type=SearchFieldDataType.DateTimeOffset),\n",
    "        SearchableField(name=\"page_number\", type=SearchFieldDataType.Int32),\n",
    "        SearchableField(name=\"total_pages\", type=SearchFieldDataType.Int32),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "        SearchField(name=\"embeddings\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=1536, vector_search_configuration=search_vector_config_name),\n",
    "    ]\n",
    "    \n",
    "    vector_search = VectorSearch(\n",
    "        algorithm_configurations=[\n",
    "            HnswVectorSearchAlgorithmConfiguration(\n",
    "                name=search_vector_config_name,\n",
    "                kind=\"hnsw\",\n",
    "                parameters={\n",
    "                    \"m\": 4,\n",
    "                    \"efConstruction\": 400,\n",
    "                    \"efSearch\": 500,\n",
    "                    \"metric\": \"cosine\"\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=search_semantic_config_name,\n",
    "        prioritized_fields=PrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"filename\"),\n",
    "            prioritized_content_fields=[SemanticField(field_name=\"content\")]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Create the semantic settings with the configuration\n",
    "    semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "    index = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "        scoring_profiles=scoring_profiles,\n",
    "        semantic_settings=semantic_settings,\n",
    "        vector_search=vector_search,\n",
    "        cors_options=cors_options,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        upload_schema = admin_client.create_index(index)\n",
    "        if upload_schema:\n",
    "            print(\"Schema uploaded successfully.\")\n",
    "        else:\n",
    "            raise Exception(\"Schema upload failed.\")\n",
    "    except:\n",
    "        raise Exception(\"Unexpected Error. Schema upload failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(5))\n",
    "def generate_embeddings(text):\n",
    "    try:\n",
    "        logging.info(\"Generating embeddings for text of length %d\", len(text))\n",
    "        response = openai.Embedding.create(input=text, engine=embedding_model)\n",
    "        embeddings = response[\"data\"][0]['embedding']\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error generating embeddings: %s\", str(e))\n",
    "        raise\n",
    "        \n",
    "def convert_date_to_iso8601(date):\n",
    "    try:\n",
    "        date = date.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    except:\n",
    "        print(\"Error converting date to ISO8601 format\")\n",
    "        date = None\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdfs_and_upload_to_index(root_dir, client):\n",
    "    data_list = []\n",
    "    total_files = sum(len(files) for _, _, files in os.walk(root_dir))\n",
    "    count = 0\n",
    "    \n",
    "    for dirpath, dirs, files in os.walk(root_dir):\n",
    "        for filename in fnmatch.filter(files, '*.pdf'):\n",
    "            count += 1\n",
    "            logging.info(f\"Processing {filename} {count}/{total_files}\")\n",
    "\n",
    "            print(f\"Processing {filename} {count}/{total_files}\")\n",
    "            pdf_file = os.path.join(dirpath, filename)\n",
    "            \n",
    "            with open(pdf_file, 'rb') as fileobj:\n",
    "                pdf = PyPDF2.PdfReader(fileobj)\n",
    "                info = pdf.metadata\n",
    "                author = info.author\n",
    "                created_date = info.creation_date\n",
    "                mod_date = info.modification_date\n",
    "                num_pages = len(pdf.pages)\n",
    "\n",
    "                # loop through each page of the PDF\n",
    "                for i in range(num_pages):\n",
    "                    text = pdf.pages[i].extract_text()\n",
    "                    \n",
    "                    try:\n",
    "                        embeddings = generate_embeddings(text)\n",
    "                    except:\n",
    "                        logging.error(f\"Error generating embeddings for {pdf_file}\")\n",
    "\n",
    "                        print(f\"Error generating embeddings for {pdf_file}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # create unique id for each page\n",
    "                    id_str = str(count) + \"_\" + str(i)\n",
    "                    \n",
    "                    data = {\n",
    "                        \"id\": id_str,\n",
    "                        \"filename\": os.path.basename(pdf_file),\n",
    "                        \"author\": author,\n",
    "                        \"page_number\": str(i+1),\n",
    "                        \"total_pages\": str(num_pages),\n",
    "                        \"content\": text,\n",
    "                        \"embeddings\": embeddings\n",
    "                    }\n",
    "                    \n",
    "                    data[\"created_date\"] = convert_date_to_iso8601(created_date)\n",
    "                    if not data[\"created_date\"]:\n",
    "                        continue\n",
    "                    \n",
    "                    data[\"last_modified_date\"] = convert_date_to_iso8601(mod_date)\n",
    "                    if not data[\"last_modified_date\"]:\n",
    "                        continue\n",
    "\n",
    "                    data_list.append(data)\n",
    "                \n",
    "    client.upload_documents(documents=data_list)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_client = CreateClient(search_endpoint, search_service_key, search_index_name)\n",
    "search_client = base_client.create_search_client()\n",
    "admin_client = base_client.create_admin_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 17:20:28,461 - INFO - Request URL: 'https://wsl-cog-search-test-2.search.windows.net/indexes?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '1946'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=minimal'\n",
      "    'x-ms-client-request-id': 'b614d4c4-45e8-11ee-beb6-c89402167d5b'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.4.0b8 Python/3.11.4 (Windows-10-10.0.22621-SP0)'\n",
      "A body is sent with the request\n",
      "2023-08-28 17:20:29,573 - INFO - Response status: 201\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=minimal; odata.streaming=true; charset=utf-8'\n",
      "    'ETag': '\"0x8DBA80C9A5EB010\"'\n",
      "    'Location': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': 'b614d4c4-45e8-11ee-beb6-c89402167d5b'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Mon, 28 Aug 2023 21:20:28 GMT'\n",
      "2023-08-28 17:20:29,575 - INFO - Processing DE03234-23.pdf 1/10\n",
      "2023-08-28 17:20:29,592 - INFO - Generating embeddings for text of length 1563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema uploaded successfully.\n",
      "Processing DE03234-23.pdf 1/10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'convert_date_to_iso8601' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m schema \u001b[38;5;241m=\u001b[39m create_schema_from_json_and_upload(search_index_name, admin_client)\n\u001b[1;32m----> 2\u001b[0m convert_pdfs_and_upload_to_index(data_directory, search_client)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 47\u001b[0m, in \u001b[0;36mconvert_pdfs_and_upload_to_index\u001b[1;34m(root_dir, client)\u001b[0m\n\u001b[0;32m     35\u001b[0m id_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(count) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)\n\u001b[0;32m     37\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: id_str,\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(pdf_file),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: embeddings\n\u001b[0;32m     45\u001b[0m }\n\u001b[1;32m---> 47\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_date\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m convert_date_to_iso8601(created_date)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_date\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'convert_date_to_iso8601' is not defined"
     ]
    }
   ],
   "source": [
    "schema = create_schema_from_json_and_upload(search_index_name, admin_client)\n",
    "convert_pdfs_and_upload_to_index(data_directory, search_client)\n",
    "print(\"Upload complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 17:20:25,386 - INFO - Request URL: 'https://wsl-cog-search-test-2.search.windows.net/indexes('cogsrch-index')?api-version=REDACTED'\n",
      "Request method: 'DELETE'\n",
      "Request headers:\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=minimal'\n",
      "    'x-ms-client-request-id': 'b43f9fed-45e8-11ee-a689-c89402167d5b'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.4.0b8 Python/3.11.4 (Windows-10-10.0.22621-SP0)'\n",
      "No body was attached to the request\n",
      "2023-08-28 17:20:25,739 - INFO - Response status: 204\n",
      "Response headers:\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'request-id': 'b43f9fed-45e8-11ee-a689-c89402167d5b'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Mon, 28 Aug 2023 21:20:24 GMT'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index deleted\n"
     ]
    }
   ],
   "source": [
    "# Clean up Azure resources\n",
    "admin_client.delete_index(search_index_name)\n",
    "print(\"Index deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
